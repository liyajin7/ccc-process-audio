{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mp\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn as sk\n",
    "import sklearn.metrics as mt\n",
    "import sklearn.pipeline as pp\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.preprocessing as pr\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.feature_extraction.text as te\n",
    "\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pandasql import sqldf\n",
    "pysqldf = lambda q: sqldf(q, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "fmt = '%(asctime)s : %(levelname)s : %(message)s'\n",
    "logging.basicConfig(format=fmt, level=logging.INFO)\n",
    "\n",
    "logging.getLogger(\"gensim\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.expanduser('~/github/masthesis/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1511200828\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouper(it, n=None):\n",
    "    assert n is None or n > 0\n",
    "\n",
    "    if n is None:\n",
    "        yield [x for x in it]\n",
    "    else:\n",
    "        ret = []\n",
    "\n",
    "        for obj in it:\n",
    "            if len(ret) == n:\n",
    "                yield ret\n",
    "                ret = []\n",
    "\n",
    "            if len(ret) < n:\n",
    "                ret += [obj]\n",
    "\n",
    "        # at this point, we're out of\n",
    "        # objects but len(ret) < n\n",
    "        if len(ret) > 0:\n",
    "            yield ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = 'follow_community'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/radio/show-pairs-content-with-twitter-metrics.csv')\n",
    "data = data.loc[~data[dv].isna(), :]\n",
    "\n",
    "display(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[dv].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    display(data[['show_id', 'show_name', dv]]\\\n",
    "                .drop_duplicates()\\\n",
    "                .sort_values(dv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.read_csv('data/radio/ngram-vocab.csv').word.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = ms.GroupShuffleSplit(n_splits=1, train_size=0.75, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds, test_inds = next(grp.split(data, groups=data.show_id))\n",
    "\n",
    "data_train, data_test = data.iloc[train_inds, :], data.iloc[test_inds, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[dv].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[dv].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits = grp.split(data_train['content'], data_train[dv], groups=data_train['show_id'])\n",
    "\n",
    "# display(data_test[dv].value_counts())\n",
    "# assert data_test[dv].value_counts().shape[0] == data[dv].nunique()\n",
    "\n",
    "# for tr, ts in splits:\n",
    "#     trc = data_train.loc[data_train.index[tr], dv].value_counts()\n",
    "#     tsc = data_train.loc[data_train.index[ts], dv].value_counts()\n",
    "    \n",
    "#     display(trc)\n",
    "#     display(tsc)\n",
    "    \n",
    "#     assert trc.shape[0] == data[dv].nunique()\n",
    "#     assert tsc.shape[0] == data[dv].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train[dv]\n",
    "y_test = data_test[dv]\n",
    "\n",
    "y_train_bin = pr.label_binarize(data_train[dv], classes=[0, 1, 2, 3])\n",
    "y_test_bin = pr.label_binarize(data_test[dv], classes=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pp.Pipeline(steps=[\n",
    "    ('words', te.TfidfVectorizer(\n",
    "        input='content',\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 2),\n",
    "        vocabulary=vocab,\n",
    "        max_features=20000\n",
    "    )),\n",
    "    \n",
    "    ('train', lm.LogisticRegression(\n",
    "        multi_class='multinomial',\n",
    "        max_iter=1000\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data_train['content'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvpred_train = model.predict(data_train['content'])\n",
    "dvpred_proba_train = model.predict_proba(data_train['content'])\n",
    "\n",
    "dvpred_test = model.predict(data_test['content'])\n",
    "dvpred_proba_test = model.predict_proba(data_test['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_score = mt.roc_auc_score(y_train, dvpred_proba_train, multi_class='ovr', average='macro')\n",
    "oos_score = mt.roc_auc_score(y_test, dvpred_proba_test, multi_class='ovr', average='macro')\n",
    "\n",
    "print('In-sample: {0}'.format(is_score))\n",
    "print('Out-of-sample: {0}'.format(oos_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-sample diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    display(data_train.groupby('show_name')[dv].max().sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(mt.confusion_matrix(y_train, dvpred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bin = pr.label_binarize(data_train[dv], classes=[0, 1, 2, 3])\n",
    "\n",
    "tmp = [\n",
    "    mt.roc_curve(y, pred, pos_label=1)\n",
    "    for y, pred in zip(list(y_train_bin.T), list(dvpred_proba_train.T))\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "for i, (ax, (fpr, tpr, thresh)) in enumerate(zip(axes.flat, tmp)):\n",
    "    auc = mt.auc(fpr, tpr)\n",
    "    \n",
    "    ax.plot(fpr, tpr, color='darkorange',\n",
    "            lw=2, label='ROC curve (area = %0.3f)' % (auc,))\n",
    "    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "\n",
    "    ax.set_title('Community ' + str(i))\n",
    "    ax.legend(loc='lower right')\n",
    "    \n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, allaxes = plt.subplots(4, 2, figsize=(10, 10))\n",
    "\n",
    "for axes, y, pred in zip(grouper(allaxes.flat, 2), list(y_train_bin.T), list(dvpred_proba_train.T)):\n",
    "    axes[0].set_title('Ground truth')\n",
    "    axes[1].set_title('Predicted probabilities')\n",
    "\n",
    "    axes[0].set_xlim(0, 1)\n",
    "    axes[1].set_xlim(0, 1)\n",
    "\n",
    "    _ = pd.Series(y).hist(bins=50, ax=axes[0])\n",
    "    _ = pd.Series(pred).hist(bins=50, ax=axes[1])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, allaxes = plt.subplots(4, 2, figsize=(10, 10))\n",
    "\n",
    "for axes, y, pred in zip(grouper(allaxes.flat, 2), list(y_train_bin.T), list(dvpred_proba_train.T)):\n",
    "    axes[0].set_title('y = 0')\n",
    "    axes[1].set_title('y = 1')\n",
    "\n",
    "    axes[0].set_xlim(0, 1)\n",
    "    axes[1].set_xlim(0, 1)\n",
    "\n",
    "    _ = pd.Series(pred[y == 0]).hist(bins=50, ax=axes[0])\n",
    "    _ = pd.Series(pred[y == 1]).hist(bins=50, ax=axes[1])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title('Mean predicted probability by date')\n",
    "\n",
    "pd.DataFrame(dvpred_proba_train).groupby(data_train.date).mean().plot(ax=ax, rot=45)\n",
    "_ = ax.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-sample diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    display(data_test.groupby('show_name')[dv].max().sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(mt.confusion_matrix(y_test, dvpred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [\n",
    "    mt.roc_curve(y, pred, pos_label=1)\n",
    "    for y, pred in zip(list(y_test_bin.T), list(dvpred_proba_test.T))\n",
    "]\n",
    "\n",
    "priors = y_test_bin.mean(axis=0)\n",
    "names = ['Mixed', ' New York Liberals', 'DC Liberals', 'Conservatives']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "for i, (ax, prior, name, (fpr, tpr, thresh)) in enumerate(zip(axes.flat, priors, names, tmp)):\n",
    "    auc = mt.auc(fpr, tpr)\n",
    "    \n",
    "    ax.plot(fpr, tpr, color='darkorange',\n",
    "            lw=2, label='ROC curve (area = %0.3f)' % (auc,))\n",
    "    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "\n",
    "    ax.set_title('Community ' + str(i) + ': ' + name + '\\n' + str(round(prior*100, 1)) + '% of Test Set')\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "for ax in axes.flat:\n",
    "    texts = [ax.title, ax.xaxis.label, ax.yaxis.label]\n",
    "    texts += ax.get_xticklabels()\n",
    "    texts += ax.get_yticklabels()\n",
    "    texts += ax.legend().get_texts()\n",
    "    \n",
    "    for item in texts:\n",
    "        item.set_fontsize(14)\n",
    "\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, allaxes = plt.subplots(4, 2, figsize=(10, 10))\n",
    "\n",
    "for axes, y, pred in zip(grouper(allaxes.flat, 2), list(y_test_bin.T), list(dvpred_proba_test.T)):\n",
    "    axes[0].set_title('Ground truth')\n",
    "    axes[1].set_title('Predicted probabilities')\n",
    "\n",
    "    axes[0].set_xlim(0, 1)\n",
    "    axes[1].set_xlim(0, 1)\n",
    "\n",
    "    _ = pd.Series(y).hist(bins=50, ax=axes[0])\n",
    "    _ = pd.Series(pred).hist(bins=50, ax=axes[1])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, allaxes = plt.subplots(4, 2, figsize=(10, 10))\n",
    "\n",
    "for axes, y, pred in zip(grouper(allaxes.flat, 2), list(y_test_bin.T), list(dvpred_proba_test.T)):\n",
    "    axes[0].set_title('y = 0')\n",
    "    axes[1].set_title('y = 1')\n",
    "\n",
    "    axes[0].set_xlim(0, 1)\n",
    "    axes[1].set_xlim(0, 1)\n",
    "\n",
    "    _ = pd.Series(pred[y == 0]).hist(bins=50, ax=axes[0])\n",
    "    _ = pd.Series(pred[y == 1]).hist(bins=50, ax=axes[1])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title('Mean predicted probability by date')\n",
    "\n",
    "pd.DataFrame(dvpred_proba_test).groupby(data_test.date).mean().plot(ax=ax, rot=45)\n",
    "_ = ax.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_and_coefs(data, dv, model, content='content', vocabulary=None):\n",
    "    ##\n",
    "    ## Prep the features\n",
    "    ##\n",
    "    \n",
    "    words = te.TfidfVectorizer(\n",
    "        input='content',\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 2),\n",
    "        vocabulary=vocabulary,\n",
    "        \n",
    "        max_features=10000\n",
    "    )\n",
    "\n",
    "    scaler = pr.StandardScaler()\n",
    "\n",
    "    vecs = words.fit_transform(data[content])\n",
    "    vecs = np.asarray(vecs.todense())\n",
    "    vecs = scaler.fit_transform(vecs)\n",
    "\n",
    "    ##\n",
    "    ## Fit models for feature importances\n",
    "    ##\n",
    "    \n",
    "    model.fit(vecs, data[dv])\n",
    "    \n",
    "    ##\n",
    "    ## Build return dataset\n",
    "    ##\n",
    "    \n",
    "    features = pd.DataFrame(pd.Series(words.vocabulary_, name='ind')) \\\n",
    "                   .reset_index() \\\n",
    "                   .rename({'index': 'ngram'}, axis=1) \\\n",
    "                   .sort_values('ind')\n",
    "\n",
    "    if sk.base.is_regressor(model):\n",
    "        features['coef_' + dv] = model.coef_\n",
    "    elif data[dv].nunique() > 2:\n",
    "        for i, c in enumerate(model.classes_):\n",
    "            features['coef_' + dv + '_' + str(c)] = model.coef_[i, :]\n",
    "    else:\n",
    "        features['coef_' + dv] = model.coef_.T\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_and_coefs(data, dv=dv, vocabulary=vocab, model=dict(model.steps)['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 300\n",
    "\n",
    "topk = []\n",
    "botk = []\n",
    "\n",
    "for v in list(set(list(features)) - set(['ngram', 'ind'])):\n",
    "    tk = features.sort_values(v, ascending=False)\n",
    "    tk = tk.loc[:, ['ngram', v]]\n",
    "    tk = tk.rename({v: 'coef'}, axis=1)\n",
    "    tk['dv'] = v\n",
    "    topk += [tk.head(k)]\n",
    "\n",
    "    bk = features.sort_values(v, ascending=True)\n",
    "    bk = bk.loc[:, ['ngram', v]]\n",
    "    bk = bk.rename({v: 'coef'}, axis=1)\n",
    "    bk['dv'] = v\n",
    "    botk += [bk.head(k)]\n",
    "    \n",
    "topk = pd.concat(topk, axis=0)\n",
    "botk = pd.concat(botk, axis=0)\n",
    "\n",
    "topk = topk.drop_duplicates()\n",
    "botk = botk.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', lambda x: '%.15f' % x), pd.option_context('display.max_rows', None):\n",
    "    display(topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', lambda x: '%.15f' % x), pd.option_context('display.max_rows', None):\n",
    "    display(botk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recodes = {\n",
    "    'coef_follow_community_0.0': '0',\n",
    "    'coef_follow_community_1.0': '1',\n",
    "    'coef_follow_community_2.0': '2',\n",
    "    'coef_follow_community_3.0': '3',\n",
    "}\n",
    "\n",
    "tmp = topk.groupby('dv').apply(lambda x: x.sample(n=5)) \\\n",
    "          .drop('dv', axis=1).reset_index() \\\n",
    "          .drop('level_1', axis=1) \\\n",
    "          .replace(dict(dv=recodes)) \\\n",
    "          .drop('coef', axis=1)\n",
    "tmp['pos'] = tmp.groupby('dv').cumcount() + 1\n",
    "tmp = tmp.set_index(['pos', 'dv']).unstack('dv')\n",
    "tmp.columns = [x[1] for x in tmp.columns]\n",
    "\n",
    "print(tmp.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
