{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mp\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn as sk\n",
    "import sklearn.metrics as mt\n",
    "import sklearn.pipeline as pp\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.preprocessing as pr\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.feature_extraction.text as te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "fmt = '%(asctime)s : %(levelname)s : %(message)s'\n",
    "logging.basicConfig(format=fmt, level=logging.INFO)\n",
    "\n",
    "logging.getLogger(\"gensim\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.expanduser('~/github/masthesis/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1511200828\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = 'dim0_hosts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/radio/show-pairs-content-with-twitter-metrics.csv')\n",
    "\n",
    "display(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.read_csv('data/radio/ngram-vocab.csv').word.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = ms.GroupShuffleSplit(n_splits=3, train_size=0.75, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds, test_inds = next(grp.split(data, groups=data.show_id))\n",
    "\n",
    "data_train, data_test = data.iloc[train_inds, :].copy(), data.iloc[test_inds, :].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pp.Pipeline(steps=[\n",
    "    ('words', te.TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 2),\n",
    "        vocabulary=vocab,\n",
    "        max_features=20000\n",
    "    )),\n",
    "        \n",
    "    ('train', lm.LinearRegression(fit_intercept=True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data_train['content'], data_train[dv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvpred_train = pd.Series(model.predict(data_train['content']), index=data_train.index)\n",
    "dvpred_test = pd.Series(model.predict(data_test['content']), index=data_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_score = mt.r2_score(data_train[dv], dvpred_train)\n",
    "oos_score = mt.r2_score(data_test[dv], dvpred_test)\n",
    "\n",
    "print('In-sample: {0}'.format(is_score))\n",
    "print('Out-of-sample: {0}'.format(oos_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-sample diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].set_title('Predicted vs actual')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "axes[0].set_xlim(1.1 * min(-dvpred_train.std(), dvpred_train.min()), 1.1 * dvpred_train.max())\n",
    "axes[0].set_ylim(1.1 * min(-data_train[dv].std(), data_train[dv].min()), 1.1 * data_train[dv].max())\n",
    "\n",
    "axes[1].set_title('Residuals')\n",
    "\n",
    "_ = axes[0].scatter(dvpred_train, data_train[dv], s=5, alpha=0.75, c='navy', lw=0.25)\n",
    "_ = axes[1].hist(data_train[dv] - dvpred_train, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "resid_train = data_train[dv] - dvpred_train\n",
    "\n",
    "ax.set_xlim(1.1 * min(-data_train[dv].std(), data_train[dv].min()), 1.1 * data_train[dv].max())\n",
    "ax.set_ylim(1.1 * min(-resid_train.std(), resid_train.min()), 1.1 * resid_train.max())\n",
    "\n",
    "ax.set_title('Residuals vs actual')\n",
    "\n",
    "ax.scatter(data_train[dv], resid_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].set_title('Actual')\n",
    "axes[1].set_title('Predicted')\n",
    "\n",
    "axes[0].set_xlim(0, 1.1 * data_train[dv].max())\n",
    "axes[1].set_xlim(0, 1.1 * data_train[dv].max())\n",
    "\n",
    "_ = data_train[dv].hist(bins=50, ax=axes[0])\n",
    "_ = dvpred_train.hist(bins=50, ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title('Mean prediction by date')\n",
    "\n",
    "pd.DataFrame(dvpred_train).groupby(data_train.date).mean().plot(ax=ax, rot=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(15, 10), sharex=True)\n",
    "\n",
    "labels = data_train.groupby('show_id')['show_name'].max()\n",
    "\n",
    "data_train[dv].groupby(data_train['show_id']).mean().plot(kind='bar', ax=axes[0])\n",
    "dvpred_train.groupby(data_train['show_id']).mean().plot(kind='bar', ax=axes[1])\n",
    "resid_train.groupby(data_train['show_id']).mean().plot(kind='bar', ax=axes[2])\n",
    "\n",
    "axes[0].set_xticklabels(labels, rotation=90)\n",
    "axes[1].set_xticklabels(labels, rotation=90)\n",
    "axes[2].set_xticklabels(labels, rotation=90)\n",
    "\n",
    "axes[0].set_title('Actuals by show')\n",
    "axes[1].set_title('Predictions by show')\n",
    "axes[2].set_title('Residuals by show')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-sample diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].set_title('Predicted vs actual')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xlim(1.1 * dvpred_test.min(), 1.1 * dvpred_test.max())\n",
    "axes[0].set_ylim(1.1 * min(-data_test[dv].std(), data_test[dv].min()), 1.1 * data_test[dv].max())\n",
    "\n",
    "axes[1].set_title('Residuals')\n",
    "\n",
    "_ = axes[0].scatter(dvpred_test, data_test[dv], s=5, alpha=0.75, c='navy', lw=0.25)\n",
    "_ = axes[1].hist(data_test[dv] - dvpred_test, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "resid_test = data_test[dv] - dvpred_test\n",
    "\n",
    "ax.set_xlim(1.1 * min(-data_test[dv].std(), data_test[dv].min()), 1.1 * data_test[dv].max())\n",
    "ax.set_ylim(1.1 * min(-resid_test.std(), resid_test.min()), 1.1 * resid_test.max())\n",
    "\n",
    "axes[1].set_title('Residuals vs actual')\n",
    "\n",
    "ax.scatter(data_test[dv], resid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].set_title('Actual')\n",
    "axes[1].set_title('Predicted')\n",
    "\n",
    "axes[0].set_xlim(0, 1.1 * data_test[dv].max())\n",
    "axes[1].set_xlim(0, 1.1 * data_test[dv].max())\n",
    "\n",
    "_ = data_train[dv].hist(bins=50, ax=axes[0])\n",
    "_ = dvpred_test.hist(bins=50, ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title('Mean prediction by date')\n",
    "\n",
    "pd.DataFrame(dvpred_test).groupby(data_test.date).mean().plot(ax=ax, rot=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(15, 10), sharex=True)\n",
    "\n",
    "labels = data_test.groupby('show_id')['show_name'].max()\n",
    "\n",
    "data_test[dv].groupby(data_test['show_id']).mean().plot(kind='bar', ax=axes[0])\n",
    "dvpred_test.groupby(data_test['show_id']).mean().plot(kind='bar', ax=axes[1])\n",
    "resid_test.groupby(data_test['show_id']).mean().plot(kind='bar', ax=axes[2])\n",
    "\n",
    "axes[0].set_xticklabels(labels, rotation=90)\n",
    "axes[1].set_xticklabels(labels, rotation=90)\n",
    "axes[2].set_xticklabels(labels, rotation=90)\n",
    "\n",
    "axes[0].set_title('Actuals by show')\n",
    "axes[1].set_title('Predictions by show')\n",
    "axes[2].set_title('Residuals by show')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_and_coefs(data, dv, model, content='content', vocabulary=None):\n",
    "    ##\n",
    "    ## Prep the features\n",
    "    ##\n",
    "    \n",
    "    words = te.TfidfVectorizer(\n",
    "        input='content',\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 2),\n",
    "        vocabulary=vocabulary,\n",
    "        \n",
    "        max_features=10000\n",
    "    )\n",
    "\n",
    "    scaler = pr.StandardScaler()\n",
    "\n",
    "    vecs = words.fit_transform(data[content])\n",
    "    vecs = np.asarray(vecs.todense())\n",
    "    vecs = scaler.fit_transform(vecs)\n",
    "\n",
    "    ##\n",
    "    ## Fit models for feature importances\n",
    "    ##\n",
    "    \n",
    "    model.fit(vecs, data[dv])\n",
    "    \n",
    "    ##\n",
    "    ## Build return dataset\n",
    "    ##\n",
    "    \n",
    "    features = pd.DataFrame(pd.Series(words.vocabulary_, name='ind')) \\\n",
    "                   .reset_index() \\\n",
    "                   .rename({'index': 'ngram'}, axis=1) \\\n",
    "                   .sort_values('ind')\n",
    "\n",
    "    if sk.base.is_regressor(model):\n",
    "        features['coef_' + dv] = model.coef_\n",
    "    elif data[dv].nunique() > 2:\n",
    "        for i, c in enumerate(model.classes_):\n",
    "            features['coef_' + dv + '_' + str(c)] = model.coef_[i, :]\n",
    "    else:\n",
    "        features['coef_' + dv] = model.coef_.T\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_and_coefs(data, dv=dv, vocabulary=vocab, model=dict(model.steps)['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 300\n",
    "\n",
    "topk = []\n",
    "botk = []\n",
    "\n",
    "for v in list(set(list(features)) - set(['ngram', 'ind'])):\n",
    "    tk = features.sort_values(v, ascending=False)\n",
    "    tk = tk.loc[:, ['ngram', v]]\n",
    "    tk = tk.rename({v: 'coef'}, axis=1)\n",
    "    tk['dv'] = v\n",
    "    topk += [tk.head(k)]\n",
    "\n",
    "    bk = features.sort_values(v, ascending=True)\n",
    "    bk = bk.loc[:, ['ngram', v]]\n",
    "    bk = bk.rename({v: 'coef'}, axis=1)\n",
    "    bk['dv'] = v\n",
    "    botk += [bk.head(k)]\n",
    "    \n",
    "topk = pd.concat(topk, axis=0)\n",
    "botk = pd.concat(botk, axis=0)\n",
    "\n",
    "topk = topk.drop_duplicates()\n",
    "botk = botk.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', lambda x: '%.15f' % x), pd.option_context('display.max_rows', None):\n",
    "    display(topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', lambda x: '%.15f' % x), pd.option_context('display.max_rows', None):\n",
    "    display(botk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpt = topk.groupby('dv').apply(lambda x: x.sample(n=5)) \\\n",
    "           .drop('dv', axis=1).reset_index() \\\n",
    "           .drop('level_1', axis=1) \\\n",
    "           .drop('coef', axis=1)\n",
    "tmpt['pos'] = tmpt.groupby('dv').cumcount() + 1\n",
    "tmpt = tmpt.set_index(['pos', 'dv']).unstack('dv')\n",
    "tmpt.columns = [x[1] for x in tmpt.columns]\n",
    "\n",
    "tmpb = botk.groupby('dv').apply(lambda x: x.sample(n=5)) \\\n",
    "           .drop('dv', axis=1).reset_index() \\\n",
    "           .drop('level_1', axis=1) \\\n",
    "           .drop('coef', axis=1)\n",
    "tmpb['pos'] = tmpb.groupby('dv').cumcount() + 1\n",
    "tmpb = tmpb.set_index(['pos', 'dv']).unstack('dv')\n",
    "tmpb.columns = [x[1] for x in tmpb.columns]\n",
    "\n",
    "tmp = pd.concat([tmpt, tmpb], axis=1)\n",
    "tmp.columns = ['top', 'bottom']\n",
    "\n",
    "print(tmp.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
