{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mp\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn as sk\n",
    "import sklearn.metrics as mt\n",
    "import sklearn.pipeline as pp\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.preprocessing as pr\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.feature_extraction.text as te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "fmt = '%(asctime)s : %(levelname)s : %(message)s'\n",
    "logging.basicConfig(format=fmt, level=logging.INFO)\n",
    "\n",
    "logging.getLogger(\"gensim\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.expanduser('~/github/masthesis/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1511200828\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dim = 'dim0_hosts'\n",
    "dv = 'ideology'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/radio/show-pairs-content-with-twitter-metrics.csv')\n",
    "\n",
    "data[dv] = (data[target_dim] >= data[target_dim].mean()).astype(int)\n",
    "\n",
    "display(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.read_csv('data/radio/ngram-vocab.csv').word.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = ms.GroupShuffleSplit(n_splits=3, train_size=0.75, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds, test_inds = next(grp.split(data, groups=data.show_id))\n",
    "\n",
    "data_train, data_test = data.iloc[train_inds, :].copy(), data.iloc[test_inds, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    display(data[['show_id', 'show_name', dv, target_dim]]\\\n",
    "                .drop_duplicates()\\\n",
    "                .sort_values(target_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "   'train__C': np.logspace(-1, 1),\n",
    "   'train__penalty': ['l1', 'l2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pp.Pipeline(steps=[\n",
    "    ('words', te.TfidfVectorizer(\n",
    "        input='content',\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 2),\n",
    "        vocabulary=vocab,\n",
    "        max_features=20000)\n",
    "    ),\n",
    "    \n",
    "    ('train', lm.LogisticRegression(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data_train['content'], data_train[dv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.loc[:, 'dvpred'] = pd.Series(model.predict(data_train['content']), index=data_train.index)\n",
    "data_train.loc[:, 'dvpred_proba'] = pd.Series(model.predict_proba(data_train['content'])[:, 1],\n",
    "                                              index=data_train.index)\n",
    "\n",
    "data_test.loc[:, 'dvpred'] = pd.Series(model.predict(data_test['content']), index=data_test.index)\n",
    "data_test.loc[:, 'dvpred_proba'] = pd.Series(model.predict_proba(data_test['content'])[:, 1],\n",
    "                                             index=data_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_score = mt.roc_auc_score(data_train[dv], data_train['dvpred_proba'])\n",
    "oos_score = mt.roc_auc_score(data_test[dv], data_test['dvpred_proba'])\n",
    "\n",
    "print('In-sample: {0}'.format(is_score))\n",
    "print('Out-of-sample: {0}'.format(oos_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-sample diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    display(data_train.groupby('show_name')[dv].max().sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(mt.confusion_matrix(data_train[dv], data_train['dvpred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresh = mt.roc_curve(data_train[dv], data_train['dvpred_proba'], pos_label=1)\n",
    "auc = mt.auc(fpr, tpr)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(fpr, tpr, color='darkorange',\n",
    "        lw=2, label='ROC curve (area = %0.3f)' % (auc,))\n",
    "ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "\n",
    "ax.set_title('ROC curve')\n",
    "ax.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].set_title('Ground truth')\n",
    "axes[1].set_title('Predicted probabilities')\n",
    "\n",
    "axes[0].set_xlim(0, 1)\n",
    "axes[1].set_xlim(0, 1)\n",
    "\n",
    "_ = data_train[dv].hist(bins=50, ax=axes[0])\n",
    "_ = data_train['dvpred_proba'].hist(bins=50, ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].set_title('y = 0')\n",
    "axes[1].set_title('y = 1')\n",
    "\n",
    "axes[0].set_xlim(0, 1)\n",
    "axes[1].set_xlim(0, 1)\n",
    "\n",
    "_ = data_train.loc[data_train[dv] == 0, 'dvpred_proba'].hist(bins=50, ax=axes[0])\n",
    "_ = data_train.loc[data_train[dv] == 1, 'dvpred_proba'].hist(bins=50, ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title('Mean predicted probability by date')\n",
    "\n",
    "_ = data_train.groupby('date')['dvpred_proba'].mean().plot(ax=ax, rot=45)\n",
    "_ = ax.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "\n",
    "resid_train = data_train[dv] - data_train['dvpred_proba']\n",
    "labels = data_train.groupby('show_id')['show_name'].max()\n",
    "\n",
    "data_train['dvpred_proba'].groupby(data_train['show_id']).mean().plot(kind='bar', ax=axes[0])\n",
    "resid_train.groupby(data_train['show_id']).mean().plot(kind='bar', ax=axes[1])\n",
    "\n",
    "axes[0].set_xticklabels(labels)\n",
    "axes[1].set_xticklabels(labels)\n",
    "\n",
    "axes[0].set_title('Predictions by show')\n",
    "axes[1].set_title('Residuals by show')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-sample diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    display(data_test.groupby('show_name')[dv].max().sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(mt.confusion_matrix(data_test[dv], data_test['dvpred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresh = mt.roc_curve(data_test[dv], data_test['dvpred_proba'], pos_label=1)\n",
    "auc = mt.auc(fpr, tpr)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(fpr, tpr, color='darkorange',\n",
    "        lw=2, label='ROC curve (area = %0.3f)' % (auc,))\n",
    "ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "\n",
    "ax.set_title('ROC curve')\n",
    "ax.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].set_title('Ground truth')\n",
    "axes[1].set_title('Predicted probabilities')\n",
    "\n",
    "axes[0].set_xlim(0, 1)\n",
    "axes[1].set_xlim(0, 1)\n",
    "\n",
    "_ = data_test[dv].hist(bins=50, ax=axes[0])\n",
    "_ = data_test['dvpred_proba'].hist(bins=50, ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].set_title('y = 0')\n",
    "axes[1].set_title('y = 1')\n",
    "\n",
    "axes[0].set_xlim(0, 1)\n",
    "axes[1].set_xlim(0, 1)\n",
    "\n",
    "_ = data_test.loc[data_test[dv] == 0, 'dvpred_proba'].hist(bins=50, ax=axes[0])\n",
    "_ = data_test.loc[data_test[dv] == 1, 'dvpred_proba'].hist(bins=50, ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title('Mean predicted probability by date')\n",
    "\n",
    "_ = data_test.groupby('date')['dvpred_proba'].mean().plot(ax=ax, rot=45)\n",
    "_ = ax.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "\n",
    "resid_test = data_test[dv] - data_test['dvpred_proba']\n",
    "labels = data_test.groupby('show_id')['show_name'].max()\n",
    "\n",
    "data_test['dvpred_proba'].groupby(data_test['show_id']).mean().plot(kind='bar', ax=axes[0])\n",
    "resid_test.groupby(data_test['show_id']).mean().plot(kind='bar', ax=axes[1])\n",
    "\n",
    "axes[0].set_xticklabels(labels)\n",
    "axes[1].set_xticklabels(labels)\n",
    "\n",
    "axes[0].set_title('Predictions by show')\n",
    "axes[1].set_title('Residuals by show')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_and_coefs(data, dv, model, content='content', vocabulary=None):\n",
    "    ##\n",
    "    ## Prep the features\n",
    "    ##\n",
    "    \n",
    "    words = te.TfidfVectorizer(\n",
    "        input='content',\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 2),\n",
    "        vocabulary=vocabulary,\n",
    "        \n",
    "        max_features=10000\n",
    "    )\n",
    "\n",
    "    scaler = pr.StandardScaler()\n",
    "\n",
    "    vecs = words.fit_transform(data[content])\n",
    "    vecs = np.asarray(vecs.todense())\n",
    "    vecs = scaler.fit_transform(vecs)\n",
    "\n",
    "    ##\n",
    "    ## Fit models for feature importances\n",
    "    ##\n",
    "    \n",
    "    model.fit(vecs, data[dv])\n",
    "    \n",
    "    ##\n",
    "    ## Build return dataset\n",
    "    ##\n",
    "    \n",
    "    features = pd.DataFrame(pd.Series(words.vocabulary_, name='ind')) \\\n",
    "                   .reset_index() \\\n",
    "                   .rename({'index': 'ngram'}, axis=1) \\\n",
    "                   .sort_values('ind')\n",
    "\n",
    "    if sk.base.is_regressor(model):\n",
    "        features['coef_' + dv] = model.coef_\n",
    "    elif data[dv].nunique() > 2:\n",
    "        for i, c in enumerate(model.classes_):\n",
    "            features['coef_' + dv + '_' + str(c)] = model.coef_[i, :]\n",
    "    else:\n",
    "        features['coef_' + dv] = model.coef_.T\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_and_coefs(data, dv=dv, vocabulary=vocab, model=dict(model.steps)['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 300\n",
    "\n",
    "topk = []\n",
    "botk = []\n",
    "\n",
    "for v in list(set(list(features)) - set(['ngram', 'ind'])):\n",
    "    tk = features.sort_values(v, ascending=False)\n",
    "    tk = tk.loc[:, ['ngram', v]]\n",
    "    tk = tk.rename({v: 'coef'}, axis=1)\n",
    "    tk['dv'] = v\n",
    "    topk += [tk.head(k)]\n",
    "\n",
    "    bk = features.sort_values(v, ascending=True)\n",
    "    bk = bk.loc[:, ['ngram', v]]\n",
    "    bk = bk.rename({v: 'coef'}, axis=1)\n",
    "    bk['dv'] = v\n",
    "    botk += [bk.head(k)]\n",
    "    \n",
    "topk = pd.concat(topk, axis=0)\n",
    "botk = pd.concat(botk, axis=0)\n",
    "\n",
    "topk = topk.drop_duplicates()\n",
    "botk = botk.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', lambda x: '%.15f' % x), pd.option_context('display.max_rows', None):\n",
    "    display(topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', lambda x: '%.15f' % x), pd.option_context('display.max_rows', None):\n",
    "    display(botk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
