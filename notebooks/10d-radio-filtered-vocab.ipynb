{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import itertools as it\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "\n",
    "import matplotlib as mp\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn as sk\n",
    "import sklearn.multiclass as mc\n",
    "import sklearn.metrics as mt\n",
    "import sklearn.pipeline as pp\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.preprocessing as pr\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.feature_extraction.text as te\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "fmt = '%(asctime)s : %(levelname)s : %(message)s'\n",
    "logging.basicConfig(format=fmt, level=logging.INFO)\n",
    "\n",
    "logging.getLogger(\"gensim\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.expanduser('~/github/masthesis/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/radio/show-pairs-content-with-twitter-metrics.csv')\n",
    "data = data.loc[(data.date >= '2019-09-01') & (data.date <= '2019-10-31'), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_and_coefs(data, vocabulary=None, content='content'):\n",
    "    ##\n",
    "    ## Prep the features\n",
    "    ##\n",
    "    \n",
    "    words = te.TfidfVectorizer(\n",
    "        input='content',\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 2),\n",
    "        vocabulary=vocabulary,\n",
    "        \n",
    "        max_features=30000\n",
    "    )\n",
    "\n",
    "    scaler = pr.StandardScaler()\n",
    "\n",
    "    vecs = words.fit_transform(data[content])\n",
    "    vecs = np.asarray(vecs.todense())\n",
    "    vecs = scaler.fit_transform(vecs)\n",
    "\n",
    "    ##\n",
    "    ## Fit models for feature importances\n",
    "    ##\n",
    "    \n",
    "    models = {\n",
    "        'follow_community': lm.LogisticRegression(multi_class='multinomial', max_iter=1000),\n",
    "        'mention_community': lm.LogisticRegression(multi_class='multinomial', max_iter=1000),\n",
    "        'retweet_community': lm.LogisticRegression(multi_class='multinomial', max_iter=1000),\n",
    "    }\n",
    "    \n",
    "    for k, m in models.items():\n",
    "        mask = ~(data[k].isna())\n",
    "        m.fit(vecs[mask, :], data.loc[mask, k])\n",
    "\n",
    "    ##\n",
    "    ## Build return dataset\n",
    "    ##\n",
    "    \n",
    "    features = pd.DataFrame(pd.Series(words.vocabulary_, name='ind')) \\\n",
    "                   .reset_index() \\\n",
    "                   .rename({'index': 'ngram'}, axis=1) \\\n",
    "                   .sort_values('ind')\n",
    "\n",
    "    for prefix in ['follow', 'mention', 'retweet']:\n",
    "        for i, c in enumerate(models[prefix + '_community'].classes_):\n",
    "            features['coef_' + prefix + '_community_' + str(c)] = models[prefix + '_community'].coef_[i, :]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importances w/o blacklists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_and_coefs(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 300\n",
    "\n",
    "topk = []\n",
    "botk = []\n",
    "\n",
    "for dv in list(set(list(features)) - set(['ngram', 'ind'])):\n",
    "    tk = features.sort_values(dv, ascending=False)\n",
    "    tk = tk.loc[:, ['ngram', dv]]\n",
    "    tk = tk.rename({dv: 'coef'}, axis=1)\n",
    "    topk += [tk.head(k)]\n",
    "\n",
    "    bk = features.sort_values(dv, ascending=True)\n",
    "    bk = bk.loc[:, ['ngram', dv]]\n",
    "    bk = bk.rename({dv: 'coef'}, axis=1)\n",
    "    botk += [bk.head(k)]\n",
    "    \n",
    "topk = pd.concat(topk, axis=0)\n",
    "botk = pd.concat(botk, axis=0)\n",
    "\n",
    "topk = topk.drop_duplicates()\n",
    "botk = botk.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', lambda x: '%.15f' % x), pd.option_context('display.max_rows', None):\n",
    "    display(topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', lambda x: '%.15f' % x), pd.option_context('display.max_rows', None):\n",
    "    display(botk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual blacklist of n-grams\n",
    "\n",
    "These will be excluded in the TfidfVectorizer stage of language models.\n",
    "\n",
    "The idea here is that if, e.g., NPR announces \"this is NPR News,\" and that's informative about what's public radio, that isn't very interesting. We care about n-grams that reflect general differences in language, not shows shouting themselves out.\n",
    "\n",
    "There may be a more automated way to do this, but it's easy enough to do manually as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = [\n",
    "    '_a _b',\n",
    "    '_b _c',\n",
    "    '_c _a',\n",
    "    '_c news',\n",
    "    '_c news_radio',\n",
    "    '_c radio',\n",
    "    '_d _a',\n",
    "    '_f _i',\n",
    "    '_h _m',\n",
    "    '_h',\n",
    "    '_p _r',\n",
    "    '_r and',\n",
    "    '_r news',\n",
    "    '_v _u',\n",
    "    '_w _c',\n",
    "    '_y',\n",
    "    'a _b',\n",
    "    'a _c',\n",
    "    'a r'\n",
    "    'abc_news',\n",
    "    'all things',\n",
    "    'all_things considered',\n",
    "    'all_things',\n",
    "    'and kathleen_collins',\n",
    "    'and npr',\n",
    "    'and w',\n",
    "    'are i',\n",
    "    'ari_shapiro',\n",
    "    'as npr',\n",
    "    'at cpr',\n",
    "    'at k',\n",
    "    'at npr',\n",
    "    'b _b',\n",
    "    'b',\n",
    "    'bbc',\n",
    "    'brian kill',\n",
    "    'catherine',\n",
    "    'cbs',\n",
    "    'cbs_news',\n",
    "    'coast am',\n",
    "    'the_coast to',\n",
    "    'coast to',\n",
    "    'to coast',\n",
    "    'collins wealth',\n",
    "    'colorado public_radio',\n",
    "    'com support',\n",
    "    'comes from',\n",
    "    'considered from',\n",
    "    'considered',\n",
    "    'contributors include',\n",
    "    'cpr dot',\n",
    "    'dana',\n",
    "    'dave',\n",
    "    'david_greene',\n",
    "    'david_folkenflik',\n",
    "    'donnell show',\n",
    "    'dot org',\n",
    "    'eight five',\n",
    "    'eighty nine',\n",
    "    'eighty w',\n",
    "    'eighty_eight point',\n",
    "    'eighty_eight',\n",
    "    'for npr',\n",
    "    'for on_point',\n",
    "    'fox is',\n",
    "    'fox',\n",
    "    'fox_nation',\n",
    "    'fox_news radio',\n",
    "    'fox_news',\n",
    "    'fox_sports radio',\n",
    "    'fox_sports',\n",
    "    'fresh_air',\n",
    "    'from npr',\n",
    "    'g p',\n",
    "    'georgia public_broadcasting',\n",
    "    'go fox_sports',\n",
    "    'gordon deal',\n",
    "    'ground_zero',\n",
    "    'hannity',\n",
    "    'heart radio',\n",
    "    'heart radio_station',\n",
    "    'here now',\n",
    "    'i heart',\n",
    "    'is all_things',\n",
    "    'is fresh_air',\n",
    "    'is made_possible',\n",
    "    'is morning_edition',\n",
    "    'is npr',\n",
    "    'is on_point',\n",
    "    'is supported',\n",
    "    'j c',\n",
    "    'j',\n",
    "    'josh',\n",
    "    'joshua_johnson',\n",
    "    'justin'\n",
    "    'k _f',\n",
    "    'k u',\n",
    "    'k',\n",
    "    'kate',\n",
    "    'kathleen_collins wealth_management',\n",
    "    'kathleen_collins',\n",
    "    'larry',\n",
    "    'larry_elder show',\n",
    "    'larry_elder',\n",
    "    'lawrence',\n",
    "    'm david_greene',\n",
    "    'm joshua_johnson',\n",
    "    'm noel_king',\n",
    "    'm rachel',\n",
    "    'm steve_inskeep',\n",
    "    'm sam_sanders',\n",
    "    'made_possible',\n",
    "    'marketplace morning',\n",
    "    'marketplace',\n",
    "    'martin and',\n",
    "    'martin',\n",
    "    'melissa',\n",
    "    'member station',\n",
    "    'morning_edition from',\n",
    "    'morning_edition on',\n",
    "    'morning_edition',\n",
    "    'n _b'\n",
    "    'n _p',\n",
    "    'n _p',\n",
    "    'nbc_news radio',\n",
    "    'nbc_news',\n",
    "    'new_england public_radio',\n",
    "    'news dot',\n",
    "    'news network',\n",
    "    'news supported',\n",
    "    'news talk',\n",
    "    'news_radio eleven',\n",
    "    'news_radio eleventh',\n",
    "    'news_radio nine',\n",
    "    'news_radio seven',\n",
    "    'news_radio ten',\n",
    "    'news_radio',\n",
    "    'next fresh_air',\n",
    "    'nine three',\n",
    "    'ninety am',\n",
    "    'ninety dot',\n",
    "    'ninety point',\n",
    "    'ninety_one point',\n",
    "    'ninety_one',\n",
    "    'noel_king',\n",
    "    'nouri',\n",
    "    'npr and',\n",
    "    'npr comes',\n",
    "    'npr i',\n",
    "    'npr news',\n",
    "    'npr s',\n",
    "    'npr station',\n",
    "    'npr stations',\n",
    "    'npr',\n",
    "    'o m',\n",
    "    'o o',\n",
    "    'o r',\n",
    "    'o',\n",
    "    'of new_england',\n",
    "    'on fox_nation',\n",
    "    'on fox_news',\n",
    "    'on fox_sports',\n",
    "    'on ground_zero',\n",
    "    'on morning_edition',\n",
    "    'on news_radio',\n",
    "    'on npr',\n",
    "    'on point',\n",
    "    'on_point comes',\n",
    "    'on_point',\n",
    "    'one a',\n",
    "    'org and',\n",
    "    'org or',\n",
    "    'org slash',\n",
    "    'org',\n",
    "    'other contributors',\n",
    "    'p b',\n",
    "    'p r',\n",
    "    'public media',\n",
    "    'public_broadcasting',\n",
    "    'public_radio and',\n",
    "    'public_radio comes',\n",
    "    'public_radio is',\n",
    "    'public_radio was',\n",
    "    'public_radio',\n",
    "    'r dot',\n",
    "    'r g',\n",
    "    'r',\n",
    "    'rachel martin',\n",
    "    'rachel',\n",
    "    'radio lab',\n",
    "    'radio music_festival',\n",
    "    'radio_network',\n",
    "    'radiolab',\n",
    "    'rush_limbaugh',\n",
    "    's morning_edition',\n",
    "    's npr',\n",
    "    'sam_sanders',\n",
    "    'sarah',\n",
    "    'sean_hannity show',\n",
    "    'sean_hannity',\n",
    "    'six one',\n",
    "    'sixty eight',\n",
    "    'slash npr',\n",
    "    'sports_radio',\n",
    "    'stations other',\n",
    "    'stephen',\n",
    "    'steve',\n",
    "    'steve_inskeep and',\n",
    "    'steve_inskeep',\n",
    "    'support comes',\n",
    "    'support for',\n",
    "    'supported by',\n",
    "    'talk thirteen',\n",
    "    'talk_radio seventy',\n",
    "    'terry_gross',\n",
    "    'the bbc',\n",
    "    'the fox_news',\n",
    "    'the npr',\n",
    "    'the sean_hannity',\n",
    "    'the_take',\n",
    "    'things considered',\n",
    "    'thirteen eighty',\n",
    "    'this npr',\n",
    "    'this weekend_edition',\n",
    "    'three three',\n",
    "    'to all_things',\n",
    "    'to fox_news',\n",
    "    'to morning_edition',\n",
    "    'to npr',\n",
    "    'unk fox_news',\n",
    "    'unk npr',\n",
    "    'vicki',\n",
    "    'w _a',\n",
    "    'w _b',\n",
    "    'w _v',\n",
    "    'w a',\n",
    "    'w b',\n",
    "    'w i',\n",
    "    'w j',\n",
    "    'weekend_edition from',\n",
    "    'weekend_edition',\n",
    "    'westwood_one',\n",
    "    'with george',\n",
    "    'y _w',\n",
    "    'your support',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.Series(list(set(features.ngram.tolist()) - set(blacklist)), name='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.to_csv('data/radio/ngram-vocab.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importances with blacklists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_features = features_and_coefs(data, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 300\n",
    "\n",
    "topk = []\n",
    "botk = []\n",
    "\n",
    "for dv in list(set(list(bl_features)) - set(['ngram', 'ind'])):\n",
    "    tk = bl_features.sort_values(dv, ascending=False)\n",
    "    tk = tk.loc[:, ['ngram', dv]]\n",
    "    tk = tk.rename({dv: 'coef'}, axis=1)\n",
    "    topk += [tk.head(k)]\n",
    "\n",
    "    bk = bl_features.sort_values(dv, ascending=True)\n",
    "    bk = bk.loc[:, ['ngram', dv]]\n",
    "    bk = bk.rename({dv: 'coef'}, axis=1)\n",
    "    botk += [bk.head(k)]\n",
    "    \n",
    "topk = pd.concat(topk, axis=0)\n",
    "botk = pd.concat(botk, axis=0)\n",
    "\n",
    "topk = topk.drop_duplicates()\n",
    "botk = botk.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', lambda x: '%.15f' % x), pd.option_context('display.max_rows', None):\n",
    "    display(topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', lambda x: '%.15f' % x), pd.option_context('display.max_rows', None):\n",
    "    display(botk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
